#Ch12
## Notes
## Solutions
### 12.1
```c
struct atominfo_t {
    int x;
    int y;
    int z;
    float w;
} aotmInfo; //aligned or seized?

#define ATOMNUM xx
#define GRIDWIDTH xx
#define GRIDHEIGHT xx
#define BLOCKWIDTH xx
#define BLOCKHEIGHT xx

__costant__ atomInfo atominfo[ATOMNUM];

__global__
void DCSKernel(float* energygrid, int numatoms)
{
    int xindex = blockIdx.x * blockDim.x +  threadId.x;
    int yindex = blockIdx.y * blockDim.y + threadId.y;
    int outaddr = yindx * GRIDWIDTH + xindex;

    float curenergy = energygrid[outaddr];
    float coorx = gridspacing * xindex;
    float coory = gridspacing * yindex;
    int atomid;
    float energyval = 0.0f;
    for (atomid = 0; atomid < numatoms; atomid ++)
    {
        float dx = coorx - atominfo[atomid].x;
        float dy = coory - atominfo[atomid].y;
        energyval += atominfo[atomid].w 
                        * rsqrtf(dx * dx + dy * dy + atominfo[atomid].z;
    }
    energygrid[outaddr] = curenergy + energyval;
}

void (float* energygrid)
{
    dim3 blockDim(BLOCKHEIGHT, BLOCKWIDTH);
    dim3 gridDim(GRIDHEIGHT / BLOCKHEIGHT, GRIDWIDTH / BLOCKWIDTH); //suppose % == 0
    
    //Move energyid pionter by z
    DCSKernel<<<gridDim, blockDim>>>(energyid, ATOMNUM);
}
```
### 12.2
#### Figure 12.5
```c
   float curenergy = energygrid[outaddr];
    float coorx = gridspacing * xindex;
    float coory = gridspacing * yindex;
    int atomid;
    float energyval = 0.0f;
    for (atomid = 0; atomid < numatoms; atomid ++)
    {
        float dx = coorx - atominfo[atomid].x; //constant memory read +1, flop +1
        float dy = coory - atominfo[atomid].y; //constant memory read +1, flop +1
        energyval += //flop +1
            atominfo[atomid].w * //constant memory read +1, flop +1
            rsqrtf( //flop +1
            dx * dx //flop +1
            + //flop +1 
            dy * dy //flop +1
            + atominfo[atomid].z; //flop +1; constant memory read +1
    }
    energygrid[outaddr] = curenergy + energyval;
    //All cache read may be merged into 1 read
```
Altogether, constant memory read = 4 * 4;
flop = 9 * 4
#### Figure 12.7
```c
    for (atomid = 0; atomid < numatoms; atomid ++)
    {
        float dy = coory - atominfo[atomid].y;  //constant memory read +1, flop +1
        float dysqpdzsq = (dy * dy) + atominfo[atomid].z; //constant memory read1, flop +2
        float x = atominfo[atomid].x; //constant memory read +1
        float dx1 = coorx1 - x; //flop +1
        float dx2 = coorx2 - x; //flop +1
        float dx3 = coorx3 - x; //flop +1
        float dx4 = coorx4 - x; //flop +1
        float charge = atominfo[atomid].w; //constant memory read +1

        energyvalx1 += charge * rsqrtf(dx1 * dx1 + dysqpdzsq); //flop +5
        energyvalx2 += charge * rsqrtf(dx2 * dx2 + dysqpdzsq); //flop +5
        energyvalx3 += charge * rsqrtf(dx3 * dx3 + dysqpdzsq); //flop +5
        energyvalx4 += charge * rsqrtf(dx4 * dx4 + dysqpdzsq); //flop +5
    }
```
Altogether, constant memory read = 4, flop = 27
### 12.3
```c
struct atominfo_t {
    int x;
    int y;
    int z;
    float w;
} aotmInfo; //aligned or seized?

#define ATOMNUM xx
#define GRIDWIDTH xx
#define GRIDHEIGHT xx
#define BLOCKSIZEX xx
#define BLOCKSIZEY xx

__costant__ atomInfo atominfo[ATOMNUM];

__global__
void CoalescedDCS(float* energvalgrid, int numatoms, float gridspacing)
{
    int yindex = blockIdx.y * blockDim.y + threadIdx.y;
    int xindex = blockIdx.x * blockDim.x + threadIdx.x;
    int outaddr = yIndex * GRIDWIDTH + xIndex;

    float coory = gridspacing * yindex;
    float coorx = gridspacing * xindex;
    float girdspacing_coalesce = gridspacing * BLOCKSIZEX;
    int atomid;
    for (atomid = 0; atomid < numatoms; atomid ++)
    {
        float dy = coory = atominfo[atomid].y;
        float dyz2 = (dy * dy) + atominfo[atomid].z;

        float dx1 = coorx - atominfo[atomid].x;
        float dx2 = dx1 + gridspaceing_coalesce;
        float dx3 = dx2 + gridspaceing_coalesce;
        float dx4 = dx3 + gridspaceing_coalesce;
        float dx5 = dx4 + gridspaceing_coalesce;
        float dx6 = dx5 + gridspaceing_coalesce;
        float dx7 = dx6 + gridspaceing_coalesce;
        float dx8 = dx7 + gridspaceing_coalesce;

        energyvalx1 += atominfo[atomid].w * rsqrtf(dx1 * dx1 + dyz2);
        energyvalx2 += atominfo[atomid].w * rsqrtf(dx2 * dx2 + dyz2);
        energyvalx3 += atominfo[atomid].w * rsqrtf(dx3 * dx3 + dyz2);
        energyvalx4 += atominfo[atomid].w * rsqrtf(dx4 * dx4 + dyz2);
        energyvalx5 += atominfo[atomid].w * rsqrtf(dx5 * dx5 + dyz2);
        energyvalx6 += atominfo[atomid].w * rsqrtf(dx6 * dx6 + dyz2);
        energyvalx7 += atominfo[atomid].w * rsqrtf(dx7 * dx7 + dyz2);
        energyvalx8 += atominfo[atomid].w * rsqrtf(dx8 * dx8 + dyz2);

        energygrid[outaddr] += energyval1;
        energygrid[outaddr + 1 * BLOCKSIZEX] += energyval2;
        energygrid[outaddr + 2 * BLOCKSIZEX] += energyval3;
        energygrid[outaddr + 3 * BLOCKSIZEX] += energyval4;
        energygrid[outaddr + 4 * BLOCKSIZEX] += energyval5;
        energygrid[outaddr + 5 * BLOCKSIZEX] += energyval6;
        energygrid[outaddr + 6 * BLOCKSIZEX] += energyval7;
        energygrid[outaddr + 7 * BLOCKSIZEX] += energyval8;
    }
}
```
At the beginning, threads with adjacent threadIdx.x access to cells with adjacent column index. It is coalesced.
As warps are executed synchronized, all threads then move synchronized into (threadIdx.x + BLOCKSIZEX),
the column index of these cells are still adjacent so coalesced.  
### 12.4
Padding is to make threads in a block being fully used. 
That is, each thread should have corresponding cells to be accessed to.

As we decide the blockDim.x = 16 to match the warp size, and decide that each thread treat 8 cells;
a block covers 16 * 8 = 128 cells in x axis. The max((128 % x)) = 127.

While we decide the blockDim.y = 16, and each thread cover only one row of cells.
So max(16 % x) = 15.
### 12.5
1) To remove control divergence. 
2) To ensure memory coalescing.
### 12.6
1) Using more registers --> decrease number of blocks could be executed in parallel
2) Impose substantial overhead for small size of grid structure. 