==PROF== Connected to process 38906 (/home/zf/workspaces/workspace_cuda/book-cuda-c/ch3/ex02)
==PROF== Profiling "reduceUnrollData8" - 0: 0%....50%....100% - 8 passes
Starting reduce...
Cpu reduce = 2131059349: 0.059277 
Gpu unroll8 -------> <<<262144, 8>>> 
Gpu interleave 2131059349: 0.394741 
==PROF== Disconnected from process 38906
[38906] ex02@127.0.0.1
  reduceUnrollData8(int *, int *, unsigned int), 2022-Aug-16 22:20:12, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.49
    SM Frequency                                                             cycle/nsecond                           1.00
    Elapsed Cycles                                                                   cycle                        810,841
    Memory [%]                                                                           %                          37.72
    DRAM Throughput                                                                      %                          22.72
    Duration                                                                       usecond                         807.62
    L1/TEX Cache Throughput                                                              %                          59.91
    L2 Cache Throughput                                                                  %                          15.09
    SM Active Cycles                                                                 cycle                     793,638.36
    Compute (SM) [%]                                                                     %                          37.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                          8
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                     262,144
    Registers Per Thread                                                   register/thread                             26
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      2,097,152
    Waves Per SM                                                                                                   455.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             16
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          40.81
    Achieved Active Warps Per SM                                                      warp                          13.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory See the CUDA Best   
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

